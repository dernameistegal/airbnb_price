{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dernameistegal/airbnb_price/blob/main/pictures_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Preparation"
      ],
      "metadata": {
        "id": "jbtdvB6FQgvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title remove repos from disc\n",
        "%cd /content\n",
        "!rm -r airbnb_price"
      ],
      "metadata": {
        "id": "QdOQljNz3blJ",
        "outputId": "83fcb3af-d10a-401f-81ab-60b8265a5b85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ7o4vsf3Sn9",
        "cellView": "form",
        "outputId": "b4edc5da-4fe4-4d5f-d3de-760d032682bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Clone repo\n",
        "!git clone https://github.com/dernameistegal/airbnb_price.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'airbnb_price'...\n",
            "remote: Enumerating objects: 127, done.\u001b[K\n",
            "remote: Counting objects: 100% (127/127), done.\u001b[K\n",
            "remote: Compressing objects: 100% (115/115), done.\u001b[K\n",
            "remote: Total 127 (delta 58), reused 54 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (127/127), 1.90 MiB | 6.88 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title add paths to library search path\n",
        "import sys \n",
        "\n",
        "sys.path.append(\"/content/airbnb_price/custom_functions\")\n",
        "sys.path.append(\"/content/airbnb_price/feature_extraction\")"
      ],
      "metadata": {
        "id": "JwAoaaJGkz3k",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CxkNtPCI8nl",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f935323-927b-47e5-ac6f-211c31d6eb34"
      },
      "source": [
        "#@title Imports and drive\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "# own modules\n",
        "import general_utils as gu\n",
        "import feature_extraction_utils as fu\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "#@title Mount drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5FPlKWBJMWs",
        "outputId": "1b36af12-ec58-4d82-8d6a-c53f32917563",
        "cellView": "form"
      },
      "source": [
        "#@title define device\n",
        "\n",
        "# device\n",
        "device = gu.get_device()\n",
        "num_cpus = os.cpu_count()\n",
        "print(num_cpus, 'CPUs available')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda available: True ; cudnn available: True ; num devices: 1\n",
            "Using device Tesla T4\n",
            "2 CPUs available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Cleaning Hostpics (Dont has to be run again)"
      ],
      "metadata": {
        "id": "QLS2yRBCQb6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hostpics_dir = \"/content/drive/MyDrive/Colab/airbnb/data/hostpics/hostpics_raw\""
      ],
      "metadata": {
        "id": "jqlGUnqDjabS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make dataset and dataloader with hostpics\n",
        "\n",
        "# load moments\n",
        "hostpics_moments = np.load(\"/content/drive/MyDrive/Colab/airbnb/data/hostpics/hostpics_moments.npy\")\n",
        "hostpics_moments = torch.from_numpy(hostpics_moments)\n",
        "\n",
        "# initialize dataset and dataloader\n",
        "dataset = fu.Dataset(filepath=hostpics_dir, channel_moments=hostpic_moments, ndata=10)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n"
      ],
      "metadata": {
        "id": "p5h86a3dtPir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features from pretrained model\n",
        "vgg = torchvision.models.vgg19(pretrained=True)\n",
        "feature_extractor = vgg.features[0:31]\n",
        "\n",
        "# compute features for later training\n",
        "train_features = fu.compute_train_features(device=device, dataloader=dataloader, feature_extractor=feature_extractor)\n",
        "train_features = train.features.cpu().numpy()"
      ],
      "metadata": {
        "id": "OJiY3_a68RwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Feature Extraction thumbnails"
      ],
      "metadata": {
        "id": "rmwXb1Kb46Eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thumbnails_dir = \"/content/drive/MyDrive/Colab/airbnb/data/thumbnails/thumbnails_raw\"\n",
        "response_dir = \"/content/drive/MyDrive/Colab/airbnb/data/thumbnails/thumbnails_response\""
      ],
      "metadata": {
        "id": "rsbXtkQz4_uP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make train_dataset and val_dataset and respective dataloader with thumbnails\n",
        "\n",
        "# load moments\n",
        "thumbnails_moments = np.load(\"/content/drive/MyDrive/Colab/airbnb/data/thumbnails/thumbnails_moments.npy\")\n",
        "thumbnails_moments = torch.from_numpy(thumbnails_moments)\n",
        "\n",
        "# initialize dataset and dataloader\n",
        "dataset = fu.Dataset(picture_dir=thumbnails_dir, response_dir= response_dir, channel_moments=thumbnails_moments, ndata=1000)\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [700, 300], generator=torch.Generator().manual_seed(42))\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "PIqyQ8V28UMd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title define models classes\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, feature_extractor, finalizer):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.finalizer = finalizer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = self.finalizer(x)\n",
        "    \n",
        "        return x\n",
        "\n",
        "class Finalizer(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.linear1 = torch.nn.Linear(in_features = 25088, out_features=4096)\n",
        "        self.drop1 = torch.nn.Dropout()\n",
        "        self.linear2 = torch.nn.Linear(in_features=4096, out_features=2048)\n",
        "        self.drop2 = torch.nn.Dropout()\n",
        "        self.linear3 = torch.nn.Linear(in_features=2048, out_features=1024)\n",
        "        self.drop3 = torch.nn.Dropout()\n",
        "        self.linear4 = torch.nn.Linear(in_features=1024, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.pool1(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = self.drop1(x)\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = self.drop2(x)\n",
        "        x = F.relu(self.linear3(x))\n",
        "        x = self.drop3(x)\n",
        "        x = self.linear4(x)\n",
        "\n",
        "        return x\n",
        "        "
      ],
      "metadata": {
        "cellView": "form",
        "id": "-9RGDHyeNXfm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define feature extractor\n",
        "vgg = torchvision.models.vgg19(pretrained=True)\n",
        "feature_extractor = vgg.features[0:31]"
      ],
      "metadata": {
        "id": "XcP_tLZ7O5cS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define finalizer\n",
        "finalizer = Finalizer()"
      ],
      "metadata": {
        "id": "FDfsVTSfVPJw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model = Model(feature_extractor=feature_extractor, finalizer=finalizer)"
      ],
      "metadata": {
        "id": "ODLjvltkVWEz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "lSsgmS6NvdV-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze parameters in feature extractor\n",
        "for name, p in model.named_parameters():\n",
        "    if \"feature_extractor\" in name:\n",
        "        p.requires_grad = False"
      ],
      "metadata": {
        "id": "v_chLc92WAe9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define loss function and optimizer\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "AfT5rNA7YLDs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title define train functions\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, ExponentialLR, StepLR\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "import fastprogress\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "    \n",
        "def train(dataloader, optimizer, model, loss_fn, device, master_bar, scaler):\n",
        "    model.train()\n",
        "    epoch_loss = []\n",
        "\n",
        "    for image, target in fastprogress.progress_bar(dataloader, parent=master_bar):\n",
        "        \n",
        "        image, target = image.to(device), target.to(device)\n",
        "\n",
        "        # zero gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        prediction = model.forward(image)\n",
        "        prediction = torch.squeeze(prediction)\n",
        "\n",
        "        # loss calculation\n",
        "        loss = loss_fn(prediction, target)\n",
        "\n",
        "        # Backward pass\n",
        "        scaler.scale(loss).backward() #loss.backward()\n",
        "        scaler.step(optimizer) # optimizer.step()\n",
        "        scaler.update()\n",
        "\n",
        "        # For plotting the train loss, save it for each sample\n",
        "        epoch_loss.append(loss.item())\n",
        "\n",
        "    return np.mean(epoch_loss)\n",
        "\n",
        "\n",
        "def validate(dataloader, model, loss_fn, device, master_bar):\n",
        "    model.eval()\n",
        "    epoch_loss = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image, target in fastprogress.progress_bar(dataloader, parent=master_bar):\n",
        "            \n",
        "            image, target = image.to(device), target.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            prediction = model.forward(image)\n",
        "            prediction = torch.squeeze(prediction)\n",
        "\n",
        "            # loss calculation\n",
        "            loss = loss_fn(prediction, target)\n",
        "\n",
        "            # For plotting the train loss, save it for each sample\n",
        "            epoch_loss.append(loss.item())\n",
        "\n",
        "    return np.mean(epoch_loss)\n",
        "    \n",
        "\n",
        "def run_training(model, optimizer, loss_fn, device, num_epochs,\n",
        "                 train_dataloader, val_dataloader, verbose=False):\n",
        "  \n",
        "    # technical stuff\n",
        "    start_time = time.time()\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    master_bar = fastprogress.master_bar(range(num_epochs))\n",
        "\n",
        "    # instantiate losses\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "\n",
        "    for epoch in master_bar:\n",
        "\n",
        "        # Train the model\n",
        "        epoch_train_loss = train(train_dataloader, optimizer, model, loss_fn, device, master_bar, scaler)\n",
        "        #Validate the model\n",
        "        epoch_val_loss = validate(val_dataloader, model, loss_fn, device, master_bar)\n",
        "\n",
        "        # Save loss and acc for plotting\n",
        "        train_loss.append(epoch_train_loss)\n",
        "        val_loss.append(epoch_val_loss)\n",
        "\n",
        "\n",
        "        if verbose:\n",
        "            master_bar.write(\n",
        "                f'Train loss: {epoch_train_loss:.2f}, val loss: {epoch_val_loss:.2f}')\n",
        "\n",
        "    time_elapsed = np.round(time.time() - start_time, 0).astype(int)\n",
        "    print(f'Finished training after {time_elapsed} seconds.')\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def plot(title, label, train_results, val_results, yscale='linear', save_path=None, \n",
        "         extra_pt=None, extra_pt_label=None):\n",
        "    \"\"\"Plot learning curves.\n",
        "\n",
        "    Args:\n",
        "        title (str): Title of plot\n",
        "      \n",
        "\n",
        "  label (str): x-axis label\n",
        "        train_results (list): Results vector of training of length of number\n",
        "            of epochs trained. Could be loss or accuracy.\n",
        "        val_results (list): Results vector of validation of length of number\n",
        "            of epochs. Could be loss or accuracy.\n",
        "        yscale (str, optional): Matplotlib.pyplot.yscale parameter. \n",
        "            Defaults to 'linear'.\n",
        "        save_path (str, optional): If passed, figure will be saved at this path.\n",
        "            Defaults to None.\n",
        "        extra_pt (tuple, optional): Tuple of length 2, defining x and y coordinate\n",
        "            of where an additional black dot will be plotted. Defaults to None.\n",
        "        extra_pt_label (str, optional): Legend label of extra point. Defaults to None.\n",
        "    \"\"\"\n",
        "    \n",
        "    epoch_array = np.arange(len(train_results)) + 1\n",
        "    train_label, val_label = \"Training \"+label.lower(), \"Validation \"+label.lower()\n",
        "    \n",
        "    sns.set(style='ticks')\n",
        "\n",
        "    plt.plot(epoch_array, train_results, epoch_array, val_results, linestyle='dashed', marker='o', zorder=-1)\n",
        "    legend = ['Train results', 'Validation results']\n",
        "    \n",
        "    if extra_pt:\n",
        "        ####################\n",
        "        ## YOUR CODE HERE ##\n",
        "        ####################\n",
        "        plt.scatter(extra_pt[0], extra_pt[1], c=\"k\")\n",
        "        legend = ['Train results', 'Validation results', extra_pt_label]\n",
        "\n",
        "        # END OF YOUR CODE #\n",
        "        \n",
        "    plt.legend(legend)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(label)\n",
        "    plt.yscale(yscale)\n",
        "    plt.title(title)\n",
        "    \n",
        "    # sns.despine(trim=True, offset=5)\n",
        "    plt.title(title, fontsize=15)\n",
        "    if save_path:\n",
        "        plt.savefig(str(save_path), bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def empty_loss(*irgendwas):\n",
        "    return torch.tensor(0)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EXv7vzLvXghr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, val_loss = run_training(model=model, optimizer=optimizer, loss_fn=loss_fn, device=device, num_epochs=200,\n",
        "                                    train_dataloader=train_dataloader, val_dataloader=val_dataloader, verbose=True)"
      ],
      "metadata": {
        "id": "WLE_PzsPaVyx",
        "outputId": "a5aa6e63-3952-4ee2-a7c4-77c04601c72f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='68' class='' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      34.00% [68/200 16:06<31:15]\n",
              "    </div>\n",
              "    \n",
              "Train loss: 2959.34, val loss: 5760.90<p>Train loss: 3229.58, val loss: 5486.32<p>Train loss: 2870.03, val loss: 5353.38<p>Train loss: 2982.71, val loss: 5920.47<p>Train loss: 2925.58, val loss: 5806.91<p>Train loss: 3052.61, val loss: 5613.21<p>Train loss: 2893.85, val loss: 5392.20<p>Train loss: 3204.55, val loss: 5158.69<p>Train loss: 2986.30, val loss: 5342.87<p>Train loss: 3924.48, val loss: 5162.11<p>Train loss: 3139.87, val loss: 4958.62<p>Train loss: 3241.47, val loss: 4702.04<p>Train loss: 3220.93, val loss: 5239.67<p>Train loss: 3032.89, val loss: 5218.32<p>Train loss: 3223.80, val loss: 5029.45<p>Train loss: 3044.14, val loss: 5209.07<p>Train loss: 3021.69, val loss: 5063.82<p>Train loss: 3090.75, val loss: 5433.30<p>Train loss: 3022.92, val loss: 5425.58<p>Train loss: 3141.39, val loss: 5466.73<p>Train loss: 2931.02, val loss: 4732.94<p>Train loss: 2943.61, val loss: 5658.80<p>Train loss: 2886.79, val loss: 5284.07<p>Train loss: 3969.15, val loss: 5332.69<p>Train loss: 3043.85, val loss: 6357.38<p>Train loss: 3142.50, val loss: 5483.37<p>Train loss: 3046.02, val loss: 5766.09<p>Train loss: 3071.01, val loss: 5242.56<p>Train loss: 3018.76, val loss: 4855.66<p>Train loss: 2883.53, val loss: 4974.14<p>Train loss: 2852.83, val loss: 5290.01<p>Train loss: 2966.35, val loss: 5500.15<p>Train loss: 2788.61, val loss: 5265.14<p>Train loss: 2737.17, val loss: 5331.68<p>Train loss: 2903.05, val loss: 4990.19<p>Train loss: 2774.76, val loss: 5168.32<p>Train loss: 2802.00, val loss: 5335.95<p>Train loss: 3372.88, val loss: 5267.01<p>Train loss: 2971.41, val loss: 5412.73<p>Train loss: 2811.74, val loss: 4944.85<p>Train loss: 2892.12, val loss: 4973.69<p>Train loss: 2773.25, val loss: 5536.89<p>Train loss: 2949.25, val loss: 5299.62<p>Train loss: 2853.42, val loss: 5297.72<p>Train loss: 2799.57, val loss: 5127.98<p>Train loss: 2795.06, val loss: 5199.79<p>Train loss: 2678.78, val loss: 4978.47<p>Train loss: 2742.65, val loss: 5360.72<p>Train loss: 2820.11, val loss: 5284.32<p>Train loss: 2737.64, val loss: 5438.14<p>Train loss: 2891.77, val loss: 5024.11<p>Train loss: 2747.03, val loss: 5335.99<p>Train loss: 2927.95, val loss: 5140.14<p>Train loss: 2902.35, val loss: 5488.04<p>Train loss: 2641.58, val loss: 5148.11<p>Train loss: 2646.22, val loss: 5440.45<p>Train loss: 2646.06, val loss: 5268.86<p>Train loss: 2652.91, val loss: 5406.15<p>Train loss: 2690.43, val loss: 5072.58<p>Train loss: 2632.09, val loss: 5710.02<p>Train loss: 2852.95, val loss: 5027.18<p>Train loss: 2714.99, val loss: 5007.11<p>Train loss: 2782.77, val loss: 5095.81<p>Train loss: 2889.85, val loss: 5597.01<p>Train loss: 2575.88, val loss: 5561.02<p>Train loss: 2676.79, val loss: 5028.69<p>Train loss: 2596.45, val loss: 5313.84<p>Train loss: 2782.57, val loss: 5070.09<p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='22' class='' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      50.00% [22/44 00:05<00:05]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}