{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dernameistegal/airbnb_price/blob/main/models/pictures_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Preparation"
      ],
      "metadata": {
        "id": "jbtdvB6FQgvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title remove repos from disc\n",
        "%cd /content\n",
        "!rm -r airbnb_price"
      ],
      "metadata": {
        "id": "QdOQljNz3blJ",
        "outputId": "90ce7c03-622d-4237-e974-7f6d8cfd72ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "rm: cannot remove 'airbnb_price': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ7o4vsf3Sn9",
        "cellView": "form",
        "outputId": "ea031272-bf32-4d28-8ab3-9db763fc48a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Clone repo\n",
        "!git clone https://github.com/dernameistegal/airbnb_price.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'airbnb_price'...\n",
            "remote: Enumerating objects: 232, done.\u001b[K\n",
            "remote: Counting objects: 100% (232/232), done.\u001b[K\n",
            "remote: Compressing objects: 100% (211/211), done.\u001b[K\n",
            "remote: Total 232 (delta 108), reused 93 (delta 15), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (232/232), 2.13 MiB | 7.32 MiB/s, done.\n",
            "Resolving deltas: 100% (108/108), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title add paths to library search path\n",
        "import sys \n",
        "\n",
        "sys.path.append(\"/content/airbnb_price/custom_functions\")"
      ],
      "metadata": {
        "id": "JwAoaaJGkz3k",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CxkNtPCI8nl",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4b7146a-6dae-4cba-ce24-8be2ad2d53ab"
      },
      "source": [
        "#@title Imports and drive\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "# own modules\n",
        "import general_utils as gu\n",
        "import picture_model_utils as pu\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "#@title Mount drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5FPlKWBJMWs",
        "outputId": "b450222f-31a0-4376-dfbc-d9ac6accf42b",
        "cellView": "form"
      },
      "source": [
        "#@title define device\n",
        "\n",
        "# device\n",
        "device = gu.get_device()\n",
        "num_cpus = os.cpu_count()\n",
        "print(num_cpus, 'CPUs available')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda available: True ; cudnn available: True ; num devices: 1\n",
            "Using device Tesla P100-PCIE-16GB\n",
            "2 CPUs available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Extract train features if this should be required in the future"
      ],
      "metadata": {
        "id": "QLS2yRBCQb6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hostpics_dir = \"/content/drive/MyDrive/Colab/airbnb/data/hostpics/hostpics_raw\""
      ],
      "metadata": {
        "id": "jqlGUnqDjabS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make dataset and dataloader with hostpics\n",
        "\n",
        "# load moments\n",
        "hostpics_moments = np.load(\"/content/drive/MyDrive/Colab/airbnb/data/hostpics/hostpics_moments.npy\")\n",
        "hostpics_moments = torch.from_numpy(hostpics_moments)\n",
        "\n",
        "# initialize dataset and dataloader\n",
        "dataset = fu.Dataset(filepath=hostpics_dir, channel_moments=hostpic_moments, ndata=10)              # here the dataset from picture_transformation_utils has to be chosen tho\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n"
      ],
      "metadata": {
        "id": "p5h86a3dtPir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features from pretrained model\n",
        "vgg = torchvision.models.vgg19(pretrained=True)\n",
        "feature_extractor = vgg.features[0:31]\n",
        "\n",
        "# compute features for later training\n",
        "train_features = fu.compute_train_features(device=device, dataloader=dataloader, feature_extractor=feature_extractor)\n",
        "train_features = train.features.cpu().numpy()"
      ],
      "metadata": {
        "id": "OJiY3_a68RwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model thumbnail pictures"
      ],
      "metadata": {
        "id": "rmwXb1Kb46Eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thumbnails_dir = \"/content/drive/MyDrive/Colab/airbnb/data/thumbnails/thumbnails_raw\"\n",
        "response_dir = \"/content/drive/MyDrive/Colab/airbnb/data/thumbnails/thumbnails_response\""
      ],
      "metadata": {
        "id": "rsbXtkQz4_uP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndata = len(os.listdir(thumbnails_dir))"
      ],
      "metadata": {
        "id": "8TstvHftiwP3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make train_dataset and val_dataset and respective dataloader with thumbnails\n",
        "\n",
        "# load moments\n",
        "thumbnails_moments = np.load(\"/content/drive/MyDrive/Colab/airbnb/data/thumbnails/thumbnails_moments.npy\")\n",
        "thumbnails_moments = torch.from_numpy(thumbnails_moments)\n",
        "\n",
        "# initialize dataset and dataloader\n",
        "dataset = pu.Dataset(picture_dir=thumbnails_dir, response_dir= response_dir, channel_moments=thumbnails_moments, ndata=ndata)\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [8000, 3402], generator=torch.Generator().manual_seed(42))\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "PIqyQ8V28UMd"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title define models classes\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, feature_extractor, finalizer):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.finalizer = finalizer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = self.finalizer(x)\n",
        "    \n",
        "        return x\n",
        "\n",
        "class Finalizer(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(512)\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(512)\n",
        "        self.conv3 = torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = torch.nn.BatchNorm2d(512)\n",
        "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.linear1 = torch.nn.Linear(in_features = 25088, out_features=4096)\n",
        "        self.bn4 = torch.nn.BatchNorm1d(4096)\n",
        "        self.drop1 = torch.nn.Dropout()\n",
        "        self.linear2 = torch.nn.Linear(in_features=4096, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn1(F.relu(self.conv1(x)))\n",
        "        x = self.bn2(F.relu(self.conv2(x)))\n",
        "        x = self.bn3(F.relu(self.conv3(x)))\n",
        "        x = self.pool1(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.bn4(F.relu(self.linear1(x)))\n",
        "        x = self.drop1(x)\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        return x\n",
        "        "
      ],
      "metadata": {
        "cellView": "form",
        "id": "-9RGDHyeNXfm"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define feature extractor\n",
        "vgg = torchvision.models.vgg19(pretrained=True)\n",
        "feature_extractor = vgg.features[0:31]"
      ],
      "metadata": {
        "id": "XcP_tLZ7O5cS"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define finalizer\n",
        "finalizer = Finalizer()"
      ],
      "metadata": {
        "id": "FDfsVTSfVPJw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model = Model(feature_extractor=feature_extractor, finalizer=finalizer)"
      ],
      "metadata": {
        "id": "ODLjvltkVWEz"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "lSsgmS6NvdV-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze parameters in feature extractor\n",
        "for name, p in model.named_parameters():\n",
        "    if \"feature_extractor\" in name:\n",
        "        p.requires_grad = False"
      ],
      "metadata": {
        "id": "v_chLc92WAe9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define loss function and optimizer\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "AfT5rNA7YLDs"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title define train functions\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, ExponentialLR, StepLR\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "import fastprogress\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "    \n",
        "def train(dataloader, optimizer, model, loss_fn, device, master_bar, scaler):\n",
        "    model.train()\n",
        "    epoch_loss = []\n",
        "\n",
        "    for image, target in fastprogress.progress_bar(dataloader, parent=master_bar):\n",
        "        \n",
        "        image, target = image.to(device), target.to(device)\n",
        "\n",
        "        # zero gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        prediction = model.forward(image)\n",
        "        prediction = torch.squeeze(prediction)\n",
        "\n",
        "        # loss calculation\n",
        "        loss = loss_fn(prediction, target)\n",
        "\n",
        "        # Backward pass\n",
        "        scaler.scale(loss).backward() #loss.backward()\n",
        "        scaler.step(optimizer) # optimizer.step()\n",
        "        scaler.update()\n",
        "\n",
        "        # For plotting the train loss, save it for each sample\n",
        "        epoch_loss.append(loss.item())\n",
        "\n",
        "    return np.mean(epoch_loss)\n",
        "\n",
        "\n",
        "def validate(dataloader, model, loss_fn, device, master_bar):\n",
        "    model.eval()\n",
        "    epoch_loss = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image, target in fastprogress.progress_bar(dataloader, parent=master_bar):\n",
        "            \n",
        "            image, target = image.to(device), target.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            prediction = model.forward(image)\n",
        "            prediction = torch.squeeze(prediction)\n",
        "\n",
        "            # loss calculation\n",
        "            loss = loss_fn(prediction, target)\n",
        "\n",
        "            # For plotting the train loss, save it for each sample\n",
        "            epoch_loss.append(loss.item())\n",
        "\n",
        "    return np.mean(epoch_loss)\n",
        "    \n",
        "\n",
        "def run_training(model, optimizer, loss_fn, device, num_epochs,\n",
        "                 train_dataloader, val_dataloader, verbose=False):\n",
        "  \n",
        "    # technical stuff\n",
        "    start_time = time.time()\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    master_bar = fastprogress.master_bar(range(num_epochs))\n",
        "\n",
        "    # instantiate losses\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "\n",
        "    for epoch in master_bar:\n",
        "\n",
        "        # Train the model\n",
        "        epoch_train_loss = train(train_dataloader, optimizer, model, loss_fn, device, master_bar, scaler)\n",
        "        #Validate the model\n",
        "        epoch_val_loss = validate(val_dataloader, model, loss_fn, device, master_bar)\n",
        "\n",
        "        # Save loss and acc for plotting\n",
        "        train_loss.append(epoch_train_loss)\n",
        "        val_loss.append(epoch_val_loss)\n",
        "\n",
        "\n",
        "        if verbose:\n",
        "            master_bar.write(\n",
        "                f'Train root mse: {np.sqrt(epoch_train_loss):.4f}, val root mse: {np.sqrt(epoch_val_loss):.4f}')\n",
        "\n",
        "    time_elapsed = np.round(time.time() - start_time, 0).astype(int)\n",
        "    print(f'Finished training after {time_elapsed} seconds.')\n",
        "    return train_loss, val_loss\n",
        "    "
      ],
      "metadata": {
        "cellView": "form",
        "id": "EXv7vzLvXghr"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, val_loss = run_training(model=model, optimizer=optimizer, loss_fn=loss_fn, device=device, num_epochs=100,\n",
        "                                    train_dataloader=train_dataloader, val_dataloader=val_dataloader, verbose=True)"
      ],
      "metadata": {
        "id": "WLE_PzsPaVyx",
        "outputId": "3d9dc834-2789-4631-e4cf-7c2b89113db4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='8' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      8.00% [8/100 14:12<2:43:19]\n",
              "    </div>\n",
              "    \n",
              "Train root mse: 0.7693, val root mse: 0.6820<p>Train root mse: 0.7621, val root mse: 0.6939<p>Train root mse: 0.7310, val root mse: 0.6911<p>Train root mse: 0.7128, val root mse: 0.6726<p>Train root mse: 0.7108, val root mse: 0.6861<p>Train root mse: 0.6911, val root mse: 0.7005<p>Train root mse: 0.6749, val root mse: 0.7004<p>Train root mse: 0.6801, val root mse: 0.7177<p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='25' class='' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      92.59% [25/27 00:28<00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}